{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0de575d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
    "\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import time\n",
    "import asyncio\n",
    "\n",
    "# C·∫•u h√¨nh hi·ªÉn th·ªã\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7184bdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: H√†m ƒë·ªçc file .jsonl\n",
    "def read_jsonl(file_path, max_records=None): \n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(tqdm(f, desc=f\"Reading {os.path.basename(file_path)}\")):\n",
    "            if max_records and i >= max_records:\n",
    "                break\n",
    "            try:\n",
    "                data.append(json.loads(line))\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"JSON error at line {i}: {e}\")\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72a89af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C√°c file JSONL c√≥ trong th∆∞ m·ª•c:\n",
      "- enwiki_namespace_0_0.jsonl\n",
      "- enwiki_namespace_0_1.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: ƒê∆∞·ªùng d·∫´n t·ªõi th∆∞ m·ª•c ch·ª©a c√°c file JSONL\n",
    "folder_path = r'G:\\Ph√¢n t√≠ch CTG\\New\\BTL\\data'\n",
    "\n",
    "# L·∫•y danh s√°ch file .jsonl\n",
    "jsonl_files = [f for f in os.listdir(folder_path) if f.endswith('.jsonl')]\n",
    "\n",
    "# Ki·ªÉm tra file\n",
    "print(\"C√°c file JSONL c√≥ trong th∆∞ m·ª•c:\")\n",
    "for f in jsonl_files:\n",
    "    print(\"-\", f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd00caa6",
   "metadata": {},
   "source": [
    "### L·∫•y file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d0f467",
   "metadata": {},
   "source": [
    "#### H√†m skip d√≤ng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a392753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonl(filepath, max_records=None, skip=0):\n",
    "    records = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        # B·ªè qua s·ªë d√≤ng ƒë·∫ßu ti√™n n·∫øu c·∫ßn\n",
    "        for _ in range(skip):\n",
    "            next(f, None)\n",
    "\n",
    "        # ƒê·ªçc s·ªë l∆∞·ª£ng d√≤ng mong mu·ªën\n",
    "        for i, line in enumerate(f):\n",
    "            if max_records is not None and i >= max_records:\n",
    "                break\n",
    "            try:\n",
    "                records.append(json.loads(line))\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    return records\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ed81e8",
   "metadata": {},
   "source": [
    "#### ƒê·ªçc file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77aeb1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ghi 15000 d√≤ng v√†o Doc_file_Jsonl\\File_tho\\File_tho_batch_1.csv\n",
      "‚úÖ Ghi 15000 d√≤ng v√†o Doc_file_Jsonl\\File_tho\\File_tho_batch_2.csv\n",
      "‚úÖ Ghi 15000 d√≤ng v√†o Doc_file_Jsonl\\File_tho\\File_tho_batch_3.csv\n",
      "‚úÖ Ghi 15000 d√≤ng v√†o Doc_file_Jsonl\\File_tho\\File_tho_batch_4.csv\n",
      "‚úÖ Ghi 15000 d√≤ng v√†o Doc_file_Jsonl\\File_tho\\File_tho_batch_5.csv\n",
      "‚úÖ Ghi 15000 d√≤ng v√†o Doc_file_Jsonl\\File_tho\\File_tho_batch_6.csv\n",
      "‚úÖ Ghi 15000 d√≤ng v√†o Doc_file_Jsonl\\File_tho\\File_tho_batch_7.csv\n",
      "‚úÖ Ghi 15000 d√≤ng v√†o Doc_file_Jsonl\\File_tho\\File_tho_batch_8.csv\n",
      "‚úÖ Ghi 15000 d√≤ng v√†o Doc_file_Jsonl\\File_tho\\File_tho_batch_9.csv\n",
      "‚úÖ Ghi 15000 d√≤ng v√†o Doc_file_Jsonl\\File_tho\\File_tho_batch_10.csv\n",
      "‚úÖ Ghi 15000 d√≤ng v√†o Doc_file_Jsonl\\File_tho\\File_tho_batch_11.csv\n",
      "‚úÖ Ghi 15000 d√≤ng v√†o Doc_file_Jsonl\\File_tho\\File_tho_batch_12.csv\n",
      "‚úÖ Ghi 15000 d√≤ng v√†o Doc_file_Jsonl\\File_tho\\File_tho_batch_13.csv\n",
      "‚úÖ Ghi 15000 d√≤ng v√†o Doc_file_Jsonl\\File_tho\\File_tho_batch_14.csv\n",
      "‚úÖ Ghi 15000 d√≤ng v√†o Doc_file_Jsonl\\File_tho\\File_tho_batch_15.csv\n",
      "‚úÖ Ghi 15000 d√≤ng v√†o Doc_file_Jsonl\\File_tho\\File_tho_batch_16.csv\n",
      "‚úÖ Ghi 15000 d√≤ng v√†o Doc_file_Jsonl\\File_tho\\File_tho_batch_17.csv\n",
      "‚úÖ Ghi 15000 d√≤ng v√†o Doc_file_Jsonl\\File_tho\\File_tho_batch_18.csv\n",
      "‚úÖ Ghi 15000 d√≤ng v√†o Doc_file_Jsonl\\File_tho\\File_tho_batch_19.csv\n",
      "‚úÖ Ghi 15000 d√≤ng v√†o Doc_file_Jsonl\\File_tho\\File_tho_batch_20.csv\n",
      "‚úÖ Ghi 9509 d√≤ng v√†o Doc_file_Jsonl\\File_tho\\File_tho_batch_21.csv\n",
      "üî• T·ªïng s·ªë file th√¥ ƒë√£ t·∫°o: 21\n"
     ]
    }
   ],
   "source": [
    "# === Ph·∫ßn 1: ƒê·ªçc .jsonl v√† ghi th√†nh c√°c file CSV (ch∆∞a l·ªçc c·ªôt) ===\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# H√†m ƒë·ªçc t·ª´ng batch t·ª´ file JSONL\n",
    "def read_jsonl(path, max_records, skip):\n",
    "    results = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i < skip:\n",
    "                continue\n",
    "            if i >= skip + max_records:\n",
    "                break\n",
    "            try:\n",
    "                results.append(json.loads(line))\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"L·ªói d√≤ng {i + 1}: {e}\")\n",
    "    return results\n",
    "\n",
    "# C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n\n",
    "folder_path = r'G:\\Ph√¢n t√≠ch CTG\\New\\BTL\\data'\n",
    "jsonl_file = 'enwiki_namespace_0_0.jsonl'\n",
    "file_to_read = os.path.join(folder_path, jsonl_file)\n",
    "\n",
    "# T·∫°o th∆∞ m·ª•c l∆∞u file CSV th√¥\n",
    "raw_base_folder = \"Doc_file_Jsonl\"\n",
    "raw_sub_folder = \"File_tho\"\n",
    "raw_output_dir = os.path.join(raw_base_folder, raw_sub_folder)\n",
    "os.makedirs(raw_output_dir, exist_ok=True)\n",
    "\n",
    "# C·∫•u h√¨nh batch\n",
    "records_per_file = 15000\n",
    "batch_number = 1\n",
    "skip = 0\n",
    "csv_raw_files = []\n",
    "\n",
    "while True:\n",
    "    data = read_jsonl(file_to_read, max_records=records_per_file, skip=skip)\n",
    "    if not data:\n",
    "        break\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    output_path = os.path.join(raw_output_dir, f\"File_tho_batch_{batch_number}.csv\")\n",
    "    df.to_csv(output_path, index=False)\n",
    "    csv_raw_files.append(output_path)\n",
    "    print(f\"‚úÖ Ghi {len(df)} d√≤ng v√†o {output_path}\")\n",
    "    \n",
    "    batch_number += 1\n",
    "    skip += records_per_file\n",
    "\n",
    "if not csv_raw_files:\n",
    "    print(\"‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu ƒë∆∞·ª£c ƒë·ªçc.\")\n",
    "else:\n",
    "    print(f\"üî• T·ªïng s·ªë file th√¥ ƒë√£ t·∫°o: {len(csv_raw_files)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee403846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ l∆∞u 15000 d√≤ng ƒë√£ l·ªçc v√†o: Doc_file_Jsonl\\File_loc\\File_loc_batch_1.csv\n",
      "‚úÖ ƒê√£ l∆∞u 15000 d√≤ng ƒë√£ l·ªçc v√†o: Doc_file_Jsonl\\File_loc\\File_loc_batch_2.csv\n",
      "‚úÖ ƒê√£ l∆∞u 15000 d√≤ng ƒë√£ l·ªçc v√†o: Doc_file_Jsonl\\File_loc\\File_loc_batch_3.csv\n",
      "‚úÖ ƒê√£ l∆∞u 15000 d√≤ng ƒë√£ l·ªçc v√†o: Doc_file_Jsonl\\File_loc\\File_loc_batch_4.csv\n",
      "‚úÖ ƒê√£ l∆∞u 15000 d√≤ng ƒë√£ l·ªçc v√†o: Doc_file_Jsonl\\File_loc\\File_loc_batch_5.csv\n",
      "‚úÖ ƒê√£ l∆∞u 15000 d√≤ng ƒë√£ l·ªçc v√†o: Doc_file_Jsonl\\File_loc\\File_loc_batch_6.csv\n",
      "‚úÖ ƒê√£ l∆∞u 15000 d√≤ng ƒë√£ l·ªçc v√†o: Doc_file_Jsonl\\File_loc\\File_loc_batch_7.csv\n",
      "‚úÖ ƒê√£ l∆∞u 15000 d√≤ng ƒë√£ l·ªçc v√†o: Doc_file_Jsonl\\File_loc\\File_loc_batch_8.csv\n",
      "‚úÖ ƒê√£ l∆∞u 15000 d√≤ng ƒë√£ l·ªçc v√†o: Doc_file_Jsonl\\File_loc\\File_loc_batch_9.csv\n",
      "‚úÖ ƒê√£ l∆∞u 15000 d√≤ng ƒë√£ l·ªçc v√†o: Doc_file_Jsonl\\File_loc\\File_loc_batch_10.csv\n",
      "‚úÖ ƒê√£ l∆∞u 15000 d√≤ng ƒë√£ l·ªçc v√†o: Doc_file_Jsonl\\File_loc\\File_loc_batch_11.csv\n",
      "‚úÖ ƒê√£ l∆∞u 15000 d√≤ng ƒë√£ l·ªçc v√†o: Doc_file_Jsonl\\File_loc\\File_loc_batch_12.csv\n",
      "‚úÖ ƒê√£ l∆∞u 15000 d√≤ng ƒë√£ l·ªçc v√†o: Doc_file_Jsonl\\File_loc\\File_loc_batch_13.csv\n",
      "‚úÖ ƒê√£ l∆∞u 15000 d√≤ng ƒë√£ l·ªçc v√†o: Doc_file_Jsonl\\File_loc\\File_loc_batch_14.csv\n",
      "‚úÖ ƒê√£ l∆∞u 15000 d√≤ng ƒë√£ l·ªçc v√†o: Doc_file_Jsonl\\File_loc\\File_loc_batch_15.csv\n",
      "‚úÖ ƒê√£ l∆∞u 15000 d√≤ng ƒë√£ l·ªçc v√†o: Doc_file_Jsonl\\File_loc\\File_loc_batch_16.csv\n",
      "‚úÖ ƒê√£ l∆∞u 15000 d√≤ng ƒë√£ l·ªçc v√†o: Doc_file_Jsonl\\File_loc\\File_loc_batch_17.csv\n",
      "‚úÖ ƒê√£ l∆∞u 15000 d√≤ng ƒë√£ l·ªçc v√†o: Doc_file_Jsonl\\File_loc\\File_loc_batch_18.csv\n",
      "‚úÖ ƒê√£ l∆∞u 15000 d√≤ng ƒë√£ l·ªçc v√†o: Doc_file_Jsonl\\File_loc\\File_loc_batch_19.csv\n",
      "‚úÖ ƒê√£ l∆∞u 15000 d√≤ng ƒë√£ l·ªçc v√†o: Doc_file_Jsonl\\File_loc\\File_loc_batch_20.csv\n",
      "‚úÖ ƒê√£ l∆∞u 9509 d√≤ng ƒë√£ l·ªçc v√†o: Doc_file_Jsonl\\File_loc\\File_loc_batch_21.csv\n",
      "üéâ Ho√†n t·∫•t l·ªçc! T·ªïng s·ªë file ƒë√£ t·∫°o: 21\n"
     ]
    }
   ],
   "source": [
    "# === Ph·∫ßn 2: L·ªçc d·ªØ li·ªáu t·ª´ CSV th√¥ v√† ghi ra CSV m·ªõi ===\n",
    "filtered_base_folder = \"Doc_file_Jsonl\"\n",
    "filtered_sub_folder = \"File_loc\"\n",
    "filtered_output_dir = os.path.join(filtered_base_folder, filtered_sub_folder)\n",
    "os.makedirs(filtered_output_dir, exist_ok=True)\n",
    "\n",
    "columns_to_keep = ['name', 'abstract', 'sections']\n",
    "csv_filtered_files = []\n",
    "filtered_batch = 1\n",
    "\n",
    "for raw_file in csv_raw_files:\n",
    "    df = pd.read_csv(raw_file)\n",
    "    \n",
    "    # L·ªçc c√°c c·ªôt c·∫ßn thi·∫øt\n",
    "    df_filtered = df[columns_to_keep].copy()\n",
    "    df_filtered = df_filtered.dropna(subset=['name'])\n",
    "    df_filtered = df_filtered[df_filtered['name'].str.strip() != '']\n",
    "    \n",
    "    # Ghi ra CSV m·ªõi\n",
    "    output_path = os.path.join(filtered_output_dir, f\"File_loc_batch_{filtered_batch}.csv\")\n",
    "    df_filtered.to_csv(output_path, index=False)\n",
    "    csv_filtered_files.append(output_path)\n",
    "    \n",
    "    print(f\"‚úÖ ƒê√£ l∆∞u {len(df_filtered)} d√≤ng ƒë√£ l·ªçc v√†o: {output_path}\")\n",
    "    filtered_batch += 1\n",
    "\n",
    "print(f\"üéâ Ho√†n t·∫•t l·ªçc! T·ªïng s·ªë file ƒë√£ t·∫°o: {len(csv_filtered_files)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca745f3",
   "metadata": {},
   "source": [
    "### T·∫°o file crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37510b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Processing CSV file: Doc_file_Jsonl\\File_loc\\File_loc_batch_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling Doc_file_Jsonl\\File_loc\\File_loc_batch_1.csv: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14941/14941 [19:58<00:00, 12.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Crawl xong Doc_file_Jsonl\\File_loc\\File_loc_batch_1.csv!\n",
      "\n",
      "üìÑ Processing CSV file: Doc_file_Jsonl\\File_loc\\File_loc_batch_10.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling Doc_file_Jsonl\\File_loc\\File_loc_batch_10.csv: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15000/15000 [24:58<00:00, 10.01it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Crawl xong Doc_file_Jsonl\\File_loc\\File_loc_batch_10.csv!\n",
      "\n",
      "üìÑ Processing CSV file: Doc_file_Jsonl\\File_loc\\File_loc_batch_11.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling Doc_file_Jsonl\\File_loc\\File_loc_batch_11.csv: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15000/15000 [23:19<00:00, 10.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Crawl xong Doc_file_Jsonl\\File_loc\\File_loc_batch_11.csv!\n",
      "\n",
      "üìÑ Processing CSV file: Doc_file_Jsonl\\File_loc\\File_loc_batch_12.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling Doc_file_Jsonl\\File_loc\\File_loc_batch_12.csv: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15000/15000 [21:04<00:00, 11.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Crawl xong Doc_file_Jsonl\\File_loc\\File_loc_batch_12.csv!\n",
      "\n",
      "üìÑ Processing CSV file: Doc_file_Jsonl\\File_loc\\File_loc_batch_13.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling Doc_file_Jsonl\\File_loc\\File_loc_batch_13.csv: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15000/15000 [19:57<00:00, 12.53it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Crawl xong Doc_file_Jsonl\\File_loc\\File_loc_batch_13.csv!\n",
      "\n",
      "üìÑ Processing CSV file: Doc_file_Jsonl\\File_loc\\File_loc_batch_14.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling Doc_file_Jsonl\\File_loc\\File_loc_batch_14.csv: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15000/15000 [18:57<00:00, 13.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Crawl xong Doc_file_Jsonl\\File_loc\\File_loc_batch_14.csv!\n",
      "\n",
      "üìÑ Processing CSV file: Doc_file_Jsonl\\File_loc\\File_loc_batch_15.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling Doc_file_Jsonl\\File_loc\\File_loc_batch_15.csv: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15000/15000 [19:52<00:00, 12.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Crawl xong Doc_file_Jsonl\\File_loc\\File_loc_batch_15.csv!\n",
      "\n",
      "üìÑ Processing CSV file: Doc_file_Jsonl\\File_loc\\File_loc_batch_16.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling Doc_file_Jsonl\\File_loc\\File_loc_batch_16.csv: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15000/15000 [24:41<00:00, 10.12it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Crawl xong Doc_file_Jsonl\\File_loc\\File_loc_batch_16.csv!\n",
      "\n",
      "üìÑ Processing CSV file: Doc_file_Jsonl\\File_loc\\File_loc_batch_17.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling Doc_file_Jsonl\\File_loc\\File_loc_batch_17.csv: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15000/15000 [23:29<00:00, 10.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Crawl xong Doc_file_Jsonl\\File_loc\\File_loc_batch_17.csv!\n",
      "\n",
      "üìÑ Processing CSV file: Doc_file_Jsonl\\File_loc\\File_loc_batch_18.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling Doc_file_Jsonl\\File_loc\\File_loc_batch_18.csv: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15000/15000 [23:29<00:00, 10.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Crawl xong Doc_file_Jsonl\\File_loc\\File_loc_batch_18.csv!\n",
      "\n",
      "üìÑ Processing CSV file: Doc_file_Jsonl\\File_loc\\File_loc_batch_19.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling Doc_file_Jsonl\\File_loc\\File_loc_batch_19.csv: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15000/15000 [23:30<00:00, 10.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Crawl xong Doc_file_Jsonl\\File_loc\\File_loc_batch_19.csv!\n",
      "\n",
      "üìÑ Processing CSV file: Doc_file_Jsonl\\File_loc\\File_loc_batch_2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling Doc_file_Jsonl\\File_loc\\File_loc_batch_2.csv: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15000/15000 [23:47<00:00, 10.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Crawl xong Doc_file_Jsonl\\File_loc\\File_loc_batch_2.csv!\n",
      "\n",
      "üìÑ Processing CSV file: Doc_file_Jsonl\\File_loc\\File_loc_batch_20.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling Doc_file_Jsonl\\File_loc\\File_loc_batch_20.csv: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15000/15000 [23:30<00:00, 10.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Crawl xong Doc_file_Jsonl\\File_loc\\File_loc_batch_20.csv!\n",
      "\n",
      "üìÑ Processing CSV file: Doc_file_Jsonl\\File_loc\\File_loc_batch_21.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling Doc_file_Jsonl\\File_loc\\File_loc_batch_21.csv: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9509/9509 [14:52<00:00, 10.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Crawl xong Doc_file_Jsonl\\File_loc\\File_loc_batch_21.csv!\n",
      "\n",
      "üìÑ Processing CSV file: Doc_file_Jsonl\\File_loc\\File_loc_batch_3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling Doc_file_Jsonl\\File_loc\\File_loc_batch_3.csv: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15000/15000 [23:19<00:00, 10.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Crawl xong Doc_file_Jsonl\\File_loc\\File_loc_batch_3.csv!\n",
      "\n",
      "üìÑ Processing CSV file: Doc_file_Jsonl\\File_loc\\File_loc_batch_4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling Doc_file_Jsonl\\File_loc\\File_loc_batch_4.csv: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15000/15000 [23:23<00:00, 10.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Crawl xong Doc_file_Jsonl\\File_loc\\File_loc_batch_4.csv!\n",
      "\n",
      "üìÑ Processing CSV file: Doc_file_Jsonl\\File_loc\\File_loc_batch_5.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling Doc_file_Jsonl\\File_loc\\File_loc_batch_5.csv: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15000/15000 [23:31<00:00, 10.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Crawl xong Doc_file_Jsonl\\File_loc\\File_loc_batch_5.csv!\n",
      "\n",
      "üìÑ Processing CSV file: Doc_file_Jsonl\\File_loc\\File_loc_batch_6.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling Doc_file_Jsonl\\File_loc\\File_loc_batch_6.csv: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15000/15000 [23:19<00:00, 10.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Crawl xong Doc_file_Jsonl\\File_loc\\File_loc_batch_6.csv!\n",
      "\n",
      "üìÑ Processing CSV file: Doc_file_Jsonl\\File_loc\\File_loc_batch_7.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling Doc_file_Jsonl\\File_loc\\File_loc_batch_7.csv: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15000/15000 [23:14<00:00, 10.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Crawl xong Doc_file_Jsonl\\File_loc\\File_loc_batch_7.csv!\n",
      "\n",
      "üìÑ Processing CSV file: Doc_file_Jsonl\\File_loc\\File_loc_batch_8.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling Doc_file_Jsonl\\File_loc\\File_loc_batch_8.csv: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15000/15000 [23:02<00:00, 10.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Crawl xong Doc_file_Jsonl\\File_loc\\File_loc_batch_8.csv!\n",
      "\n",
      "üìÑ Processing CSV file: Doc_file_Jsonl\\File_loc\\File_loc_batch_9.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling Doc_file_Jsonl\\File_loc\\File_loc_batch_9.csv: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15000/15000 [23:08<00:00, 10.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Crawl xong Doc_file_Jsonl\\File_loc\\File_loc_batch_9.csv!\n",
      "‚úÖ Crawl xong to√†n b·ªô c√°c file CSV!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ==== C·∫•u h√¨nh ====\n",
    "START_DATE = '20200101'\n",
    "END_DATE = '20241231'\n",
    "LANG = 'en'\n",
    "MAX_CONCURRENT = 12\n",
    "REQUESTS_PER_MINUTE = 100\n",
    "BATCH_SIZE = 100\n",
    "# ===================\n",
    "\n",
    "import os\n",
    "import time\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "import gc\n",
    "from aiohttp import TCPConnector\n",
    "from tqdm.asyncio import tqdm\n",
    "\n",
    "# B·ªô gi·ªõi h·∫°n t·ªëc ƒë·ªô\n",
    "class RateLimiter:\n",
    "    def __init__(self, calls: int, period: float):\n",
    "        self.calls = calls\n",
    "        self.period = period\n",
    "        self.requests = []\n",
    "    \n",
    "    async def __aenter__(self):\n",
    "        now = time.time()\n",
    "        self.requests = [t for t in self.requests if now - t < self.period]\n",
    "        if len(self.requests) >= self.calls:\n",
    "            await asyncio.sleep(self.period - (now - self.requests[0]))\n",
    "        self.requests.append(time.time())\n",
    "        return self\n",
    "    \n",
    "    async def __aexit__(self, exc_type, exc, tb):\n",
    "        pass\n",
    "\n",
    "async def get_pageviews(session: aiohttp.ClientSession, article_title: str, start_date: str, end_date: str, lang: str, max_retry: int = 3) -> list:\n",
    "    article_slug = article_title.replace(\" \", \"_\")\n",
    "    url = f\"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/{lang}.wikipedia.org/all-access/user/{article_slug}/daily/{start_date}/{end_date}\"\n",
    "    headers = {\"User-Agent\": \"MyWikiCrawler/1.0 (+mailto:cgdlddhm@gmail.com)\"}\n",
    "    retries = 0\n",
    "    while retries < max_retry:\n",
    "        async with RateLimiter(calls=REQUESTS_PER_MINUTE, period=60):\n",
    "            try:\n",
    "                async with session.get(url, headers=headers, timeout=10) as response:\n",
    "                    if response.status == 200:\n",
    "                        data = (await response.json()).get('items', [])\n",
    "                        return [{\n",
    "                            'title': article_title,\n",
    "                            'date': item['timestamp'][:8],\n",
    "                            'views': item['views']\n",
    "                        } for item in data]\n",
    "                    elif response.status == 429:\n",
    "                        wait_time = int(response.headers.get(\"Retry-After\", 10))\n",
    "                        await asyncio.sleep(wait_time)\n",
    "                        retries += 1\n",
    "                    else:\n",
    "                        return None\n",
    "            except Exception as e:\n",
    "                await asyncio.sleep(5)\n",
    "                retries += 1\n",
    "    return None\n",
    "\n",
    "async def crawl_one(session: aiohttp.ClientSession, title: str) -> tuple:\n",
    "    result = await get_pageviews(session, title, start_date=START_DATE, end_date=END_DATE, lang=LANG)\n",
    "    return title, result\n",
    "\n",
    "async def crawl_for_csv(csv_file: str, batch_number: int):\n",
    "    crawl_output_dir = os.path.join(\"Crawl(2)\", \"Crawl_raw\")\n",
    "    os.makedirs(crawl_output_dir, exist_ok=True)\n",
    "\n",
    "    output_csv = os.path.join(crawl_output_dir, f\"Crawl_raw_batch_{batch_number}.csv\")\n",
    "\n",
    "    try:\n",
    "        df_filtered = pd.read_csv(csv_file)\n",
    "    except Exception as e:\n",
    "        return\n",
    "    \n",
    "    titles = df_filtered['name'].dropna().unique().tolist()\n",
    "    if not titles:\n",
    "        return\n",
    "\n",
    "    connector = TCPConnector(limit=MAX_CONCURRENT, force_close=True)\n",
    "    semaphore = asyncio.Semaphore(MAX_CONCURRENT)\n",
    "\n",
    "    async with aiohttp.ClientSession(connector=connector) as session:\n",
    "        async def limited_crawl(title):\n",
    "            async with semaphore:\n",
    "                return await crawl_one(session, title)\n",
    "\n",
    "        tasks = [limited_crawl(title) for title in titles]\n",
    "        results = []\n",
    "\n",
    "        for i, future in enumerate(tqdm(asyncio.as_completed(tasks), total=len(tasks), desc=f\"Crawling {csv_file}\")):\n",
    "            title, result = await future\n",
    "            if result:\n",
    "                results.append(pd.DataFrame(result))\n",
    "\n",
    "            if (i + 1) % BATCH_SIZE == 0 or i == len(tasks) - 1:\n",
    "                if results:\n",
    "                    batch_df = pd.concat(results, ignore_index=True)\n",
    "                    if os.path.exists(output_csv):\n",
    "                        batch_df.to_csv(output_csv, mode='a', index=False, header=False)\n",
    "                    else:\n",
    "                        batch_df.to_csv(output_csv, index=False)\n",
    "\n",
    "                    del results\n",
    "                    del batch_df\n",
    "                    results = []\n",
    "                    gc.collect()\n",
    "\n",
    "    print(f\"‚úÖ Crawl xong {csv_file}!\")\n",
    "\n",
    "async def main():\n",
    "    filtered_dir = os.path.join(\"Doc_file_Jsonl\", \"File_loc\")\n",
    "    if not os.path.exists(filtered_dir):\n",
    "        return\n",
    "\n",
    "    csv_files = [os.path.join(filtered_dir, f) for f in os.listdir(filtered_dir) if f.endswith(\".csv\")]\n",
    "    if not csv_files:\n",
    "        return\n",
    "    \n",
    "    for batch_number, csv_file in enumerate(csv_files, 1):\n",
    "        print(f\"\\nüìÑ Processing CSV file: {csv_file}\")\n",
    "        await crawl_for_csv(csv_file, batch_number)\n",
    "    \n",
    "    print(\"‚úÖ Crawl xong to√†n b·ªô c√°c file CSV!\")\n",
    "\n",
    "# === Ch·∫°y trong Jupyter Notebook ho·∫∑c Script ===\n",
    "await main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
