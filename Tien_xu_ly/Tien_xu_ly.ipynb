{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1e9ccf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import ast\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4cefc1",
   "metadata": {},
   "source": [
    "# X√≥a nh·ªØng d√≤ng thi·∫øu gi√° tr·ªã"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4a1b01",
   "metadata": {},
   "source": [
    "##### X·ª≠ l√Ω nhi·ªÅu file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33aab67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang x·ª≠ l√Ω: Doc_file_Jsonl/File_loc\\File_loc_batch_1.csv\n",
      "ƒê√£ c·∫≠p nh·∫≠t: Doc_file_Jsonl/File_loc\\File_loc_batch_1.csv (x√≥a 0 d√≤ng)\n",
      "ƒêang x·ª≠ l√Ω: Doc_file_Jsonl/File_loc\\File_loc_batch_10.csv\n",
      "ƒê√£ c·∫≠p nh·∫≠t: Doc_file_Jsonl/File_loc\\File_loc_batch_10.csv (x√≥a 0 d√≤ng)\n",
      "ƒêang x·ª≠ l√Ω: Doc_file_Jsonl/File_loc\\File_loc_batch_11.csv\n",
      "ƒê√£ c·∫≠p nh·∫≠t: Doc_file_Jsonl/File_loc\\File_loc_batch_11.csv (x√≥a 0 d√≤ng)\n",
      "ƒêang x·ª≠ l√Ω: Doc_file_Jsonl/File_loc\\File_loc_batch_12.csv\n",
      "ƒê√£ c·∫≠p nh·∫≠t: Doc_file_Jsonl/File_loc\\File_loc_batch_12.csv (x√≥a 0 d√≤ng)\n",
      "ƒêang x·ª≠ l√Ω: Doc_file_Jsonl/File_loc\\File_loc_batch_13.csv\n",
      "ƒê√£ c·∫≠p nh·∫≠t: Doc_file_Jsonl/File_loc\\File_loc_batch_13.csv (x√≥a 0 d√≤ng)\n",
      "ƒêang x·ª≠ l√Ω: Doc_file_Jsonl/File_loc\\File_loc_batch_14.csv\n",
      "ƒê√£ c·∫≠p nh·∫≠t: Doc_file_Jsonl/File_loc\\File_loc_batch_14.csv (x√≥a 0 d√≤ng)\n",
      "ƒêang x·ª≠ l√Ω: Doc_file_Jsonl/File_loc\\File_loc_batch_15.csv\n",
      "ƒê√£ c·∫≠p nh·∫≠t: Doc_file_Jsonl/File_loc\\File_loc_batch_15.csv (x√≥a 0 d√≤ng)\n",
      "ƒêang x·ª≠ l√Ω: Doc_file_Jsonl/File_loc\\File_loc_batch_16.csv\n",
      "ƒê√£ c·∫≠p nh·∫≠t: Doc_file_Jsonl/File_loc\\File_loc_batch_16.csv (x√≥a 0 d√≤ng)\n",
      "ƒêang x·ª≠ l√Ω: Doc_file_Jsonl/File_loc\\File_loc_batch_17.csv\n",
      "ƒê√£ c·∫≠p nh·∫≠t: Doc_file_Jsonl/File_loc\\File_loc_batch_17.csv (x√≥a 0 d√≤ng)\n",
      "ƒêang x·ª≠ l√Ω: Doc_file_Jsonl/File_loc\\File_loc_batch_18.csv\n",
      "ƒê√£ c·∫≠p nh·∫≠t: Doc_file_Jsonl/File_loc\\File_loc_batch_18.csv (x√≥a 0 d√≤ng)\n",
      "ƒêang x·ª≠ l√Ω: Doc_file_Jsonl/File_loc\\File_loc_batch_19.csv\n",
      "ƒê√£ c·∫≠p nh·∫≠t: Doc_file_Jsonl/File_loc\\File_loc_batch_19.csv (x√≥a 0 d√≤ng)\n",
      "ƒêang x·ª≠ l√Ω: Doc_file_Jsonl/File_loc\\File_loc_batch_2.csv\n",
      "ƒê√£ c·∫≠p nh·∫≠t: Doc_file_Jsonl/File_loc\\File_loc_batch_2.csv (x√≥a 0 d√≤ng)\n",
      "ƒêang x·ª≠ l√Ω: Doc_file_Jsonl/File_loc\\File_loc_batch_20.csv\n",
      "ƒê√£ c·∫≠p nh·∫≠t: Doc_file_Jsonl/File_loc\\File_loc_batch_20.csv (x√≥a 0 d√≤ng)\n",
      "ƒêang x·ª≠ l√Ω: Doc_file_Jsonl/File_loc\\File_loc_batch_21.csv\n",
      "ƒê√£ c·∫≠p nh·∫≠t: Doc_file_Jsonl/File_loc\\File_loc_batch_21.csv (x√≥a 0 d√≤ng)\n",
      "ƒêang x·ª≠ l√Ω: Doc_file_Jsonl/File_loc\\File_loc_batch_3.csv\n",
      "ƒê√£ c·∫≠p nh·∫≠t: Doc_file_Jsonl/File_loc\\File_loc_batch_3.csv (x√≥a 0 d√≤ng)\n",
      "ƒêang x·ª≠ l√Ω: Doc_file_Jsonl/File_loc\\File_loc_batch_4.csv\n",
      "ƒê√£ c·∫≠p nh·∫≠t: Doc_file_Jsonl/File_loc\\File_loc_batch_4.csv (x√≥a 0 d√≤ng)\n",
      "ƒêang x·ª≠ l√Ω: Doc_file_Jsonl/File_loc\\File_loc_batch_5.csv\n",
      "ƒê√£ c·∫≠p nh·∫≠t: Doc_file_Jsonl/File_loc\\File_loc_batch_5.csv (x√≥a 0 d√≤ng)\n",
      "ƒêang x·ª≠ l√Ω: Doc_file_Jsonl/File_loc\\File_loc_batch_6.csv\n",
      "ƒê√£ c·∫≠p nh·∫≠t: Doc_file_Jsonl/File_loc\\File_loc_batch_6.csv (x√≥a 0 d√≤ng)\n",
      "ƒêang x·ª≠ l√Ω: Doc_file_Jsonl/File_loc\\File_loc_batch_7.csv\n",
      "ƒê√£ c·∫≠p nh·∫≠t: Doc_file_Jsonl/File_loc\\File_loc_batch_7.csv (x√≥a 0 d√≤ng)\n",
      "ƒêang x·ª≠ l√Ω: Doc_file_Jsonl/File_loc\\File_loc_batch_8.csv\n",
      "ƒê√£ c·∫≠p nh·∫≠t: Doc_file_Jsonl/File_loc\\File_loc_batch_8.csv (x√≥a 0 d√≤ng)\n",
      "ƒêang x·ª≠ l√Ω: Doc_file_Jsonl/File_loc\\File_loc_batch_9.csv\n",
      "ƒê√£ c·∫≠p nh·∫≠t: Doc_file_Jsonl/File_loc\\File_loc_batch_9.csv (x√≥a 0 d√≤ng)\n"
     ]
    }
   ],
   "source": [
    "# === PH·∫¶N 1: L√†m s·∫°ch d·ªØ li·ªáu File_loc ===\n",
    "# ƒê·ªãnh nghƒ©a th∆∞ m·ª•c ƒë·∫ßu v√†o v√† m·∫´u file\n",
    "input_dir = 'Doc_file_Jsonl/File_loc'\n",
    "pattern = os.path.join(input_dir, 'File_loc_batch_*.csv')\n",
    "\n",
    "# L·∫∑p qua c√°c file CSV ƒë·ªÉ l√†m s·∫°ch\n",
    "for file in glob.glob(pattern):\n",
    "    print(f\"ƒêang x·ª≠ l√Ω: {file}\")\n",
    "    df = pd.read_csv(file, dtype={'name': str, 'abstract': str, 'sections': str})\n",
    "    original_len = len(df)\n",
    "    # X√≥a c√°c d√≤ng thi·∫øu gi√° tr·ªã ·ªü c·ªôt name, abstract, sections\n",
    "    df = df.dropna(subset=['name', 'abstract', 'sections'])\n",
    "    # Ki·ªÉm tra t√≠nh h·ª£p l·ªá c·ªßa sections (ph·∫£i l√† danh s√°ch)\n",
    "    valid_sections = df['sections'].apply(lambda x: isinstance(ast.literal_eval(x), list) if pd.notnull(x) else False)\n",
    "    df = df[valid_sections]\n",
    "    # Ghi ƒë√® file g·ªëc\n",
    "    df.to_csv(file, index=False)\n",
    "    print(f\"ƒê√£ c·∫≠p nh·∫≠t: {file} (x√≥a {original_len - len(df)} d√≤ng)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7badc104",
   "metadata": {},
   "source": [
    "##### X·ª≠ l√Ω 1 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae15af13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === PH·∫¶N 1: L√†m s·∫°ch d·ªØ li·ªáu File_loc ===\n",
    "# # ƒê·ªãnh nghƒ©a th∆∞ m·ª•c ƒë·∫ßu v√†o v√† m·∫´u file\n",
    "\n",
    "# input_dir = 'Doc_file_Jsonl/File_loc'\n",
    "# file = os.path.join(input_dir, 'File_loc_batch_1.csv')\n",
    "\n",
    "# print(f\"ƒêang x·ª≠ l√Ω: {file}\")\n",
    "# df = pd.read_csv(file, dtype={'name': str, 'abstract': str, 'sections': str})\n",
    "# original_len = len(df)\n",
    "# # X√≥a c√°c d√≤ng thi·∫øu gi√° tr·ªã ·ªü c·ªôt name, abstract, sections\n",
    "# df = df.dropna(subset=['name', 'abstract', 'sections'])\n",
    "# # Ki·ªÉm tra t√≠nh h·ª£p l·ªá c·ªßa sections (ph·∫£i l√† danh s√°ch)\n",
    "# valid_sections = df['sections'].apply(lambda x: isinstance(ast.literal_eval(x), list) if pd.notnull(x) else False)\n",
    "# df = df[valid_sections]\n",
    "# # Ghi ƒë√® file g·ªëc\n",
    "# df.to_csv(file, index=False)\n",
    "# print(f\"ƒê√£ c·∫≠p nh·∫≠t: {file} (x√≥a {original_len - len(df)} d√≤ng)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26880d9c",
   "metadata": {},
   "source": [
    "# T√≠nh ƒëi·ªÉm t·ªïng h·ª£p ƒë·ªô quan tr·ªçng"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2611a5a",
   "metadata": {},
   "source": [
    "##### T√≠nh ƒëi·ªÉm nhi·ªÅu file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7408d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç X·ª≠ l√Ω File_loc_batch_1.csv...\n",
      "‚úÖ ƒê√£ l∆∞u: Diem_TF-IDF\\Diem_TF-IDF_batch_1.csv\n",
      "üîç X·ª≠ l√Ω File_loc_batch_10.csv...\n",
      "‚úÖ ƒê√£ l∆∞u: Diem_TF-IDF\\Diem_TF-IDF_batch_10.csv\n",
      "üîç X·ª≠ l√Ω File_loc_batch_11.csv...\n",
      "‚úÖ ƒê√£ l∆∞u: Diem_TF-IDF\\Diem_TF-IDF_batch_11.csv\n",
      "üîç X·ª≠ l√Ω File_loc_batch_12.csv...\n",
      "‚úÖ ƒê√£ l∆∞u: Diem_TF-IDF\\Diem_TF-IDF_batch_12.csv\n",
      "üîç X·ª≠ l√Ω File_loc_batch_13.csv...\n",
      "‚úÖ ƒê√£ l∆∞u: Diem_TF-IDF\\Diem_TF-IDF_batch_13.csv\n",
      "üîç X·ª≠ l√Ω File_loc_batch_14.csv...\n",
      "‚úÖ ƒê√£ l∆∞u: Diem_TF-IDF\\Diem_TF-IDF_batch_14.csv\n",
      "üîç X·ª≠ l√Ω File_loc_batch_15.csv...\n",
      "‚úÖ ƒê√£ l∆∞u: Diem_TF-IDF\\Diem_TF-IDF_batch_15.csv\n",
      "üîç X·ª≠ l√Ω File_loc_batch_16.csv...\n",
      "‚úÖ ƒê√£ l∆∞u: Diem_TF-IDF\\Diem_TF-IDF_batch_16.csv\n",
      "üîç X·ª≠ l√Ω File_loc_batch_17.csv...\n",
      "‚úÖ ƒê√£ l∆∞u: Diem_TF-IDF\\Diem_TF-IDF_batch_17.csv\n",
      "üîç X·ª≠ l√Ω File_loc_batch_18.csv...\n",
      "‚úÖ ƒê√£ l∆∞u: Diem_TF-IDF\\Diem_TF-IDF_batch_18.csv\n",
      "üîç X·ª≠ l√Ω File_loc_batch_19.csv...\n",
      "‚úÖ ƒê√£ l∆∞u: Diem_TF-IDF\\Diem_TF-IDF_batch_19.csv\n",
      "üîç X·ª≠ l√Ω File_loc_batch_2.csv...\n",
      "‚úÖ ƒê√£ l∆∞u: Diem_TF-IDF\\Diem_TF-IDF_batch_2.csv\n",
      "üîç X·ª≠ l√Ω File_loc_batch_20.csv...\n",
      "‚úÖ ƒê√£ l∆∞u: Diem_TF-IDF\\Diem_TF-IDF_batch_20.csv\n",
      "üîç X·ª≠ l√Ω File_loc_batch_21.csv...\n",
      "‚úÖ ƒê√£ l∆∞u: Diem_TF-IDF\\Diem_TF-IDF_batch_21.csv\n",
      "üîç X·ª≠ l√Ω File_loc_batch_3.csv...\n",
      "‚úÖ ƒê√£ l∆∞u: Diem_TF-IDF\\Diem_TF-IDF_batch_3.csv\n",
      "üîç X·ª≠ l√Ω File_loc_batch_4.csv...\n",
      "‚úÖ ƒê√£ l∆∞u: Diem_TF-IDF\\Diem_TF-IDF_batch_4.csv\n",
      "üîç X·ª≠ l√Ω File_loc_batch_5.csv...\n",
      "‚úÖ ƒê√£ l∆∞u: Diem_TF-IDF\\Diem_TF-IDF_batch_5.csv\n",
      "üîç X·ª≠ l√Ω File_loc_batch_6.csv...\n",
      "‚úÖ ƒê√£ l∆∞u: Diem_TF-IDF\\Diem_TF-IDF_batch_6.csv\n",
      "üîç X·ª≠ l√Ω File_loc_batch_7.csv...\n",
      "‚úÖ ƒê√£ l∆∞u: Diem_TF-IDF\\Diem_TF-IDF_batch_7.csv\n",
      "üîç X·ª≠ l√Ω File_loc_batch_8.csv...\n",
      "‚úÖ ƒê√£ l∆∞u: Diem_TF-IDF\\Diem_TF-IDF_batch_8.csv\n",
      "üîç X·ª≠ l√Ω File_loc_batch_9.csv...\n",
      "‚úÖ ƒê√£ l∆∞u: Diem_TF-IDF\\Diem_TF-IDF_batch_9.csv\n"
     ]
    }
   ],
   "source": [
    "# === PH·∫¶N 2: T√≠nh ƒëi·ªÉm TF-IDF ===\n",
    "# H√†m tr√≠ch xu·∫•t vƒÉn b·∫£n t·ª´ sections\n",
    "def extract_text_from_sections(sections):\n",
    "    text_parts = []\n",
    "    if isinstance(sections, list):\n",
    "        for section in sections:\n",
    "            if 'name' in section:\n",
    "                text_parts.append(str(section['name']))\n",
    "            if 'has_parts' in section:\n",
    "                for part in section['has_parts']:\n",
    "                    if isinstance(part, dict) and 'value' in part:\n",
    "                        text_parts.append(str(part['value']))\n",
    "    return ' '.join(text_parts)\n",
    "\n",
    "# H√†m k·∫øt h·ª£p vƒÉn b·∫£n t·ª´ name, abstract v√† sections\n",
    "def combine_text(row):\n",
    "    name = row['name'] if pd.notna(row['name']) else \"\"\n",
    "    abstract = row['abstract'] if pd.notna(row['abstract']) else \"\"\n",
    "    try:\n",
    "        sections = ast.literal_eval(row['sections']) if pd.notna(row['sections']) else []\n",
    "        sections_text = extract_text_from_sections(sections)\n",
    "    except Exception:\n",
    "        sections_text = \"\"\n",
    "    return f\"{name} {abstract} {sections_text}\"\n",
    "\n",
    "# T·∫°o th∆∞ m·ª•c l∆∞u ƒëi·ªÉm TF-IDF\n",
    "output_dir = 'Diem_TF-IDF'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# T·ª´ ƒëi·ªÉn l∆∞u ƒëi·ªÉm TF-IDF c·ªßa c√°c title\n",
    "valid_names_scores = {}\n",
    "\n",
    "# X·ª≠ l√Ω t·ª´ng file CSV ƒë·ªÉ t√≠nh TF-IDF\n",
    "for filename in sorted(os.listdir(input_dir)):\n",
    "    if filename.startswith('File_loc_batch_') and filename.endswith('.csv'):\n",
    "        filepath = os.path.join(input_dir, filename)\n",
    "        print(f\"üîç X·ª≠ l√Ω {filename}...\")\n",
    "        df = pd.read_csv(filepath, dtype={'name': str, 'abstract': str, 'sections': str})\n",
    "        # T·∫°o c·ªôt full_text t·ª´ name, abstract, sections\n",
    "        df['full_text'] = df.apply(combine_text, axis=1)\n",
    "        # T√≠nh TF-IDF\n",
    "        vectorizer = TfidfVectorizer(max_features=1000)\n",
    "        tfidf_matrix = vectorizer.fit_transform(df['full_text'])\n",
    "        df['tfidf_score'] = tfidf_matrix.mean(axis=1).A1\n",
    "        # Chu·∫©n h√≥a ƒëi·ªÉm TF-IDF v·ªÅ [0,1]\n",
    "        scaler = MinMaxScaler()\n",
    "        df['tfidf_score'] = scaler.fit_transform(df[['tfidf_score']])\n",
    "        # L·∫•y s·ªë batch t·ª´ t√™n file\n",
    "        batch_num = re.search(r'batch_(\\d+)', filename).group(1)\n",
    "        output_path = os.path.join(output_dir, f'Diem_TF-IDF_batch_{batch_num}.csv')\n",
    "        # L∆∞u k·∫øt qu·∫£ TF-IDF\n",
    "        df[['name', 'tfidf_score']].to_csv(output_path, index=False)\n",
    "        # L∆∞u ƒëi·ªÉm TF-IDF v√†o t·ª´ ƒëi·ªÉn\n",
    "        for _, row in df.iterrows():\n",
    "            valid_names_scores[row['name']] = row['tfidf_score']\n",
    "        print(f\"‚úÖ ƒê√£ l∆∞u: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2701c4",
   "metadata": {},
   "source": [
    "##### T√≠nh ƒëi·ªÉm 1 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdea1b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === PH·∫¶N 2: T√≠nh ƒëi·ªÉm TF-IDF ===\n",
    "# # H√†m tr√≠ch xu·∫•t vƒÉn b·∫£n t·ª´ sections\n",
    "# def extract_text_from_sections(sections):\n",
    "#     text_parts = []\n",
    "#     if isinstance(sections, list):\n",
    "#         for section in sections:\n",
    "#             if 'name' in section:\n",
    "#                 text_parts.append(str(section['name']))\n",
    "#             if 'has_parts' in section:\n",
    "#                 for part in section['has_parts']:\n",
    "#                     if isinstance(part, dict) and 'value' in part:\n",
    "#                         text_parts.append(str(part['value']))\n",
    "#     return ' '.join(text_parts)\n",
    "\n",
    "# # H√†m k·∫øt h·ª£p vƒÉn b·∫£n t·ª´ name, abstract v√† sections\n",
    "# def combine_text(row):\n",
    "#     name = row['name'] if pd.notna(row['name']) else \"\"\n",
    "#     abstract = row['abstract'] if pd.notna(row['abstract']) else \"\"\n",
    "#     try:\n",
    "#         sections = ast.literal_eval(row['sections']) if pd.notna(row['sections']) else []\n",
    "#         sections_text = extract_text_from_sections(sections)\n",
    "#     except Exception:\n",
    "#         sections_text = \"\"\n",
    "#     return f\"{name} {abstract} {sections_text}\"\n",
    "\n",
    "# # T·∫°o th∆∞ m·ª•c l∆∞u ƒëi·ªÉm TF-IDF\n",
    "# output_dir = 'Diem_TF-IDF'\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# # T·ª´ ƒëi·ªÉn l∆∞u ƒëi·ªÉm TF-IDF c·ªßa c√°c title\n",
    "# valid_names_scores = {}\n",
    "\n",
    "# # X·ª≠ l√Ω file CSV ƒë·ªÉ t√≠nh TF-IDF\n",
    "# filename = os.path.basename(file)\n",
    "# print(f\"üîç X·ª≠ l√Ω {filename}...\")\n",
    "# df = pd.read_csv(file, dtype={'name': str, 'abstract': str, 'sections': str})\n",
    "# # T·∫°o c·ªôt full_text t·ª´ name, abstract, sections\n",
    "# df['full_text'] = df.apply(combine_text, axis=1)\n",
    "# # T√≠nh TF-IDF\n",
    "# vectorizer = TfidfVectorizer(max_features=1000)\n",
    "# tfidf_matrix = vectorizer.fit_transform(df['full_text'])\n",
    "# df['tfidf_score'] = tfidf_matrix.mean(axis=1).A1\n",
    "# # Chu·∫©n h√≥a ƒëi·ªÉm TF-IDF v·ªÅ [0,1]\n",
    "# scaler = MinMaxScaler()\n",
    "# df['tfidf_score'] = scaler.fit_transform(df[['tfidf_score']])\n",
    "# # L·∫•y s·ªë batch t·ª´ t√™n file\n",
    "# batch_num = re.search(r'batch_(\\d+)', filename).group(1)\n",
    "# output_path = os.path.join(output_dir, f'Diem_TF-IDF_batch_{batch_num}.csv')\n",
    "# # L∆∞u k·∫øt qu·∫£ TF-IDF\n",
    "# df[['name', 'tfidf_score']].to_csv(output_path, index=False)\n",
    "# # L∆∞u ƒëi·ªÉm TF-IDF v√†o t·ª´ ƒëi·ªÉn\n",
    "# for _, row in df.iterrows():\n",
    "#     valid_names_scores[row['name']] = row['tfidf_score']\n",
    "# print(f\"‚úÖ ƒê√£ l∆∞u: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14a39b9",
   "metadata": {},
   "source": [
    "## File Crawl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3304d519",
   "metadata": {},
   "source": [
    "##### X·ª≠ l√Ω 1 batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a98f6534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== X·ª≠ l√Ω file: Crawl_raw_batch_4.csv ===\n",
      "Batch 4: L·ªçc t·ª´ 47640550 ‚Üí 47538893 d√≤ng\n",
      "Batch 4: Sau khi l·ªçc missing days < 30%, c√≤n 38751822 d√≤ng (10605 titles)\n",
      "üîç C·ªôt tr∆∞·ªõc khi l∆∞u: ['title', 'date', 'views', 'day_of_week', 'month', 'quarter', 'tfidf_score']\n",
      "üîç D·ªØ li·ªáu m·∫´u:\n",
      "                  title       date  views  day_of_week  month  quarter  \\\n",
      "672  Rossa Mediterranea 2024-01-01    2.0            0      1        1   \n",
      "673  Rossa Mediterranea 2024-01-02    1.0            1      1        1   \n",
      "674  Rossa Mediterranea 2024-01-03    2.0            2      1        1   \n",
      "675  Rossa Mediterranea 2024-01-04    1.0            3      1        1   \n",
      "676  Rossa Mediterranea 2024-01-05    0.0            4      1        1   \n",
      "\n",
      "     tfidf_score  \n",
      "672     0.465878  \n",
      "673     0.465878  \n",
      "674     0.465878  \n",
      "675     0.465878  \n",
      "676     0.465878  \n",
      "‚úÖ ƒê√£ l∆∞u file long cho ARIMA, TFT, Informer: Crawl(2)\\Crawl_ca_nam_long\\Crawl_full_views_ca_nam_batch_4.csv (38751822 d√≤ng)\n",
      "üéâ Ho√†n t·∫•t x·ª≠ l√Ω file!\n"
     ]
    }
   ],
   "source": [
    "# === PH·∫¶N 3: X·ª≠ l√Ω file Crawl ===\n",
    "# ƒê·ªãnh nghƒ©a th∆∞ m·ª•c v√† c·∫•u h√¨nh\n",
    "raw_folder = \"Crawl(2)/Crawl_raw\"\n",
    "output_folder = os.path.join(\"Crawl(2)\", \"Crawl_ca_nam_long\")\n",
    "\n",
    "# T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# X·ª≠ l√Ω m·ªôt file c·ª• th·ªÉ\n",
    "file = os.path.join(raw_folder, \"Crawl_raw_batch_4.csv\")\n",
    "batch_number = 4\n",
    "\n",
    "if not os.path.exists(file):\n",
    "    raise FileNotFoundError(f\"Kh√¥ng t√¨m th·∫•y file: {file}\")\n",
    "\n",
    "print(f\"\\n=== X·ª≠ l√Ω file: {os.path.basename(file)} ===\")\n",
    "\n",
    "# ƒê·ªçc v√† l·ªçc d·ªØ li·ªáu\n",
    "df = pd.read_csv(file, dtype={'title': str, 'views': 'float32'})\n",
    "df['date'] = pd.to_datetime(df['date'], format='%Y%m%d', errors='coerce')\n",
    "df.dropna(subset=['date'], inplace=True)\n",
    "\n",
    "original_len = len(df)\n",
    "df = df[df['title'].isin(valid_names_scores.keys())]\n",
    "print(f\"Batch {batch_number}: L·ªçc t·ª´ {original_len} ‚Üí {len(df)} d√≤ng\")\n",
    "\n",
    "if df.empty:\n",
    "    print(f\"Batch {batch_number}: B·ªè qua v√¨ kh√¥ng c√≥ title n√†o kh·ªõp\")\n",
    "else:\n",
    "    # === L·ªçc c√°c title c√≥ s·ªë ng√†y b·ªã thi·∫øu d∆∞·ªõi 30% ===\n",
    "    # T√≠nh kho·∫£ng ng√†y mong ƒë·ª£i (t·ª´ ng√†y nh·ªè nh·∫•t ƒë·∫øn l·ªõn nh·∫•t trong d·ªØ li·ªáu)\n",
    "    min_date = df['date'].min()\n",
    "    max_date = df['date'].max()\n",
    "    expected_days = (max_date - min_date).days + 1  # T·ªïng s·ªë ng√†y mong ƒë·ª£i\n",
    "\n",
    "    # T√≠nh s·ªë ng√†y th·ª±c t·∫ø v√† t·ª∑ l·ªá thi·∫øu cho m·ªói title\n",
    "    title_day_counts = df.groupby('title')['date'].nunique().reset_index(name='actual_days')\n",
    "    title_day_counts['missing_days'] = expected_days - title_day_counts['actual_days']\n",
    "    title_day_counts['missing_ratio'] = title_day_counts['missing_days'] / expected_days\n",
    "\n",
    "    # L·ªçc c√°c title c√≥ t·ª∑ l·ªá thi·∫øu d∆∞·ªõi 30%\n",
    "    valid_titles = title_day_counts[title_day_counts['missing_ratio'] < 0.3]['title']\n",
    "    df = df[df['title'].isin(valid_titles)]\n",
    "    print(f\"Batch {batch_number}: Sau khi l·ªçc missing days < 30%, c√≤n {len(df)} d√≤ng ({len(valid_titles)} titles)\")\n",
    "\n",
    "    if df.empty:\n",
    "        print(f\"Batch {batch_number}: B·ªè qua v√¨ kh√¥ng c√≥ title n√†o th·ªèa m√£n ƒëi·ªÅu ki·ªán missing days\")\n",
    "    else:\n",
    "        # Th√™m ƒë·∫∑c tr∆∞ng cho m√¥ h√¨nh\n",
    "        df['day_of_week'] = df['date'].dt.dayofweek.astype(np.int8)\n",
    "        df['month'] = df['date'].dt.month.astype(np.int8)\n",
    "        df['quarter'] = df['date'].dt.quarter.astype(np.int8)\n",
    "        df['tfidf_score'] = df['title'].map(valid_names_scores)\n",
    "\n",
    "        # Ki·ªÉm tra d·ªØ li·ªáu tr∆∞·ªõc khi l∆∞u\n",
    "        print(f\"üîç C·ªôt tr∆∞·ªõc khi l∆∞u: {list(df.columns)}\")\n",
    "        print(f\"üîç D·ªØ li·ªáu m·∫´u:\\n{df.head()}\")\n",
    "\n",
    "        # L∆∞u d·ªØ li·ªáu long cho c·∫£ ba m√¥ h√¨nh (ARIMA, TFT, Informer)\n",
    "        output_path = os.path.join(output_folder, f\"Crawl_full_views_ca_nam_batch_{batch_number}.csv\")\n",
    "        df[['date', 'title', 'views', 'day_of_week', 'month', 'quarter', 'tfidf_score']].to_csv(output_path, index=False)\n",
    "        print(f\"‚úÖ ƒê√£ l∆∞u file long cho ARIMA, TFT, Informer: {output_path} ({len(df)} d√≤ng)\")\n",
    "\n",
    "print(\"üéâ Ho√†n t·∫•t x·ª≠ l√Ω file!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
