{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0de575d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import các thư viện cần thiết\n",
    "\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import time\n",
    "import asyncio\n",
    "\n",
    "# Cấu hình hiển thị\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7184bdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Hàm đọc file .jsonl\n",
    "def read_jsonl(file_path, max_records=None): \n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(tqdm(f, desc=f\"Reading {os.path.basename(file_path)}\")):\n",
    "            if max_records and i >= max_records:\n",
    "                break\n",
    "            try:\n",
    "                data.append(json.loads(line))\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"JSON error at line {i}: {e}\")\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72a89af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Các file JSONL có trong thư mục:\n",
      "- enwiki_namespace_0_0.jsonl\n",
      "- enwiki_namespace_0_1.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Đường dẫn tới thư mục chứa các file JSONL\n",
    "folder_path = r'G:\\Phân tích CTG\\New\\BTL\\data'\n",
    "\n",
    "# Lấy danh sách file .jsonl\n",
    "jsonl_files = [f for f in os.listdir(folder_path) if f.endswith('.jsonl')]\n",
    "\n",
    "# Kiểm tra file\n",
    "print(\"Các file JSONL có trong thư mục:\")\n",
    "for f in jsonl_files:\n",
    "    print(\"-\", f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd00caa6",
   "metadata": {},
   "source": [
    "### Lấy file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d0f467",
   "metadata": {},
   "source": [
    "#### Hàm skip dòng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a392753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonl(filepath, max_records=None, skip=0):\n",
    "    records = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        # Bỏ qua số dòng đầu tiên nếu cần\n",
    "        for _ in range(skip):\n",
    "            next(f, None)\n",
    "\n",
    "        # Đọc số lượng dòng mong muốn\n",
    "        for i, line in enumerate(f):\n",
    "            if max_records is not None and i >= max_records:\n",
    "                break\n",
    "            try:\n",
    "                records.append(json.loads(line))\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    return records\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ed81e8",
   "metadata": {},
   "source": [
    "#### Đọc file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77aeb1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Ghi 15000 dòng vào Doc_file_Jsonl\\File_tho\\File_tho_batch_1.csv\n",
      "✅ Ghi 15000 dòng vào Doc_file_Jsonl\\File_tho\\File_tho_batch_2.csv\n",
      "✅ Ghi 15000 dòng vào Doc_file_Jsonl\\File_tho\\File_tho_batch_3.csv\n",
      "✅ Ghi 15000 dòng vào Doc_file_Jsonl\\File_tho\\File_tho_batch_4.csv\n",
      "✅ Ghi 15000 dòng vào Doc_file_Jsonl\\File_tho\\File_tho_batch_5.csv\n",
      "✅ Ghi 15000 dòng vào Doc_file_Jsonl\\File_tho\\File_tho_batch_6.csv\n",
      "✅ Ghi 15000 dòng vào Doc_file_Jsonl\\File_tho\\File_tho_batch_7.csv\n",
      "✅ Ghi 15000 dòng vào Doc_file_Jsonl\\File_tho\\File_tho_batch_8.csv\n",
      "✅ Ghi 15000 dòng vào Doc_file_Jsonl\\File_tho\\File_tho_batch_9.csv\n",
      "✅ Ghi 15000 dòng vào Doc_file_Jsonl\\File_tho\\File_tho_batch_10.csv\n",
      "✅ Ghi 15000 dòng vào Doc_file_Jsonl\\File_tho\\File_tho_batch_11.csv\n",
      "✅ Ghi 15000 dòng vào Doc_file_Jsonl\\File_tho\\File_tho_batch_12.csv\n",
      "✅ Ghi 15000 dòng vào Doc_file_Jsonl\\File_tho\\File_tho_batch_13.csv\n",
      "✅ Ghi 15000 dòng vào Doc_file_Jsonl\\File_tho\\File_tho_batch_14.csv\n",
      "✅ Ghi 15000 dòng vào Doc_file_Jsonl\\File_tho\\File_tho_batch_15.csv\n",
      "✅ Ghi 15000 dòng vào Doc_file_Jsonl\\File_tho\\File_tho_batch_16.csv\n",
      "✅ Ghi 15000 dòng vào Doc_file_Jsonl\\File_tho\\File_tho_batch_17.csv\n",
      "✅ Ghi 15000 dòng vào Doc_file_Jsonl\\File_tho\\File_tho_batch_18.csv\n",
      "✅ Ghi 15000 dòng vào Doc_file_Jsonl\\File_tho\\File_tho_batch_19.csv\n",
      "✅ Ghi 15000 dòng vào Doc_file_Jsonl\\File_tho\\File_tho_batch_20.csv\n",
      "✅ Ghi 9509 dòng vào Doc_file_Jsonl\\File_tho\\File_tho_batch_21.csv\n",
      "🔥 Tổng số file thô đã tạo: 21\n"
     ]
    }
   ],
   "source": [
    "# === Phần 1: Đọc .jsonl và ghi thành các file CSV (chưa lọc cột) ===\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Hàm đọc từng batch từ file JSONL\n",
    "def read_jsonl(path, max_records, skip):\n",
    "    results = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i < skip:\n",
    "                continue\n",
    "            if i >= skip + max_records:\n",
    "                break\n",
    "            try:\n",
    "                results.append(json.loads(line))\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Lỗi dòng {i + 1}: {e}\")\n",
    "    return results\n",
    "\n",
    "# Cấu hình đường dẫn\n",
    "folder_path = r'G:\\Phân tích CTG\\New\\BTL\\data'\n",
    "jsonl_file = 'enwiki_namespace_0_0.jsonl'\n",
    "file_to_read = os.path.join(folder_path, jsonl_file)\n",
    "\n",
    "# Tạo thư mục lưu file CSV thô\n",
    "raw_base_folder = \"Doc_file_Jsonl\"\n",
    "raw_sub_folder = \"File_tho\"\n",
    "raw_output_dir = os.path.join(raw_base_folder, raw_sub_folder)\n",
    "os.makedirs(raw_output_dir, exist_ok=True)\n",
    "\n",
    "# Cấu hình batch\n",
    "records_per_file = 15000\n",
    "batch_number = 1\n",
    "skip = 0\n",
    "csv_raw_files = []\n",
    "\n",
    "while True:\n",
    "    data = read_jsonl(file_to_read, max_records=records_per_file, skip=skip)\n",
    "    if not data:\n",
    "        break\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    output_path = os.path.join(raw_output_dir, f\"File_tho_batch_{batch_number}.csv\")\n",
    "    df.to_csv(output_path, index=False)\n",
    "    csv_raw_files.append(output_path)\n",
    "    print(f\"✅ Ghi {len(df)} dòng vào {output_path}\")\n",
    "    \n",
    "    batch_number += 1\n",
    "    skip += records_per_file\n",
    "\n",
    "if not csv_raw_files:\n",
    "    print(\"❌ Không có dữ liệu được đọc.\")\n",
    "else:\n",
    "    print(f\"🔥 Tổng số file thô đã tạo: {len(csv_raw_files)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee403846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Đã lưu 15000 dòng đã lọc vào: Doc_file_Jsonl\\File_loc\\File_loc_batch_1.csv\n",
      "✅ Đã lưu 15000 dòng đã lọc vào: Doc_file_Jsonl\\File_loc\\File_loc_batch_2.csv\n",
      "✅ Đã lưu 15000 dòng đã lọc vào: Doc_file_Jsonl\\File_loc\\File_loc_batch_3.csv\n",
      "✅ Đã lưu 15000 dòng đã lọc vào: Doc_file_Jsonl\\File_loc\\File_loc_batch_4.csv\n",
      "✅ Đã lưu 15000 dòng đã lọc vào: Doc_file_Jsonl\\File_loc\\File_loc_batch_5.csv\n",
      "✅ Đã lưu 15000 dòng đã lọc vào: Doc_file_Jsonl\\File_loc\\File_loc_batch_6.csv\n",
      "✅ Đã lưu 15000 dòng đã lọc vào: Doc_file_Jsonl\\File_loc\\File_loc_batch_7.csv\n",
      "✅ Đã lưu 15000 dòng đã lọc vào: Doc_file_Jsonl\\File_loc\\File_loc_batch_8.csv\n",
      "✅ Đã lưu 15000 dòng đã lọc vào: Doc_file_Jsonl\\File_loc\\File_loc_batch_9.csv\n",
      "✅ Đã lưu 15000 dòng đã lọc vào: Doc_file_Jsonl\\File_loc\\File_loc_batch_10.csv\n",
      "✅ Đã lưu 15000 dòng đã lọc vào: Doc_file_Jsonl\\File_loc\\File_loc_batch_11.csv\n",
      "✅ Đã lưu 15000 dòng đã lọc vào: Doc_file_Jsonl\\File_loc\\File_loc_batch_12.csv\n",
      "✅ Đã lưu 15000 dòng đã lọc vào: Doc_file_Jsonl\\File_loc\\File_loc_batch_13.csv\n",
      "✅ Đã lưu 15000 dòng đã lọc vào: Doc_file_Jsonl\\File_loc\\File_loc_batch_14.csv\n",
      "✅ Đã lưu 15000 dòng đã lọc vào: Doc_file_Jsonl\\File_loc\\File_loc_batch_15.csv\n",
      "✅ Đã lưu 15000 dòng đã lọc vào: Doc_file_Jsonl\\File_loc\\File_loc_batch_16.csv\n",
      "✅ Đã lưu 15000 dòng đã lọc vào: Doc_file_Jsonl\\File_loc\\File_loc_batch_17.csv\n",
      "✅ Đã lưu 15000 dòng đã lọc vào: Doc_file_Jsonl\\File_loc\\File_loc_batch_18.csv\n",
      "✅ Đã lưu 15000 dòng đã lọc vào: Doc_file_Jsonl\\File_loc\\File_loc_batch_19.csv\n",
      "✅ Đã lưu 15000 dòng đã lọc vào: Doc_file_Jsonl\\File_loc\\File_loc_batch_20.csv\n",
      "✅ Đã lưu 9509 dòng đã lọc vào: Doc_file_Jsonl\\File_loc\\File_loc_batch_21.csv\n",
      "🎉 Hoàn tất lọc! Tổng số file đã tạo: 21\n"
     ]
    }
   ],
   "source": [
    "# === Phần 2: Lọc dữ liệu từ CSV thô và ghi ra CSV mới ===\n",
    "filtered_base_folder = \"Doc_file_Jsonl\"\n",
    "filtered_sub_folder = \"File_loc\"\n",
    "filtered_output_dir = os.path.join(filtered_base_folder, filtered_sub_folder)\n",
    "os.makedirs(filtered_output_dir, exist_ok=True)\n",
    "\n",
    "columns_to_keep = ['name', 'abstract', 'sections']\n",
    "csv_filtered_files = []\n",
    "filtered_batch = 1\n",
    "\n",
    "for raw_file in csv_raw_files:\n",
    "    df = pd.read_csv(raw_file)\n",
    "    \n",
    "    # Lọc các cột cần thiết\n",
    "    df_filtered = df[columns_to_keep].copy()\n",
    "    df_filtered = df_filtered.dropna(subset=['name'])\n",
    "    df_filtered = df_filtered[df_filtered['name'].str.strip() != '']\n",
    "    \n",
    "    # Ghi ra CSV mới\n",
    "    output_path = os.path.join(filtered_output_dir, f\"File_loc_batch_{filtered_batch}.csv\")\n",
    "    df_filtered.to_csv(output_path, index=False)\n",
    "    csv_filtered_files.append(output_path)\n",
    "    \n",
    "    print(f\"✅ Đã lưu {len(df_filtered)} dòng đã lọc vào: {output_path}\")\n",
    "    filtered_batch += 1\n",
    "\n",
    "print(f\"🎉 Hoàn tất lọc! Tổng số file đã tạo: {len(csv_filtered_files)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca745f3",
   "metadata": {},
   "source": [
    "### Tạo file crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37510b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📄 Processing CSV file: Doc_file_Jsonl\\File_loc\\File_loc_batch_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling Doc_file_Jsonl\\File_loc\\File_loc_batch_1.csv: 100%|██████████| 14941/14941 [19:58<00:00, 12.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Crawl xong Doc_file_Jsonl\\File_loc\\File_loc_batch_1.csv!\n",
      "\n",
      "📄 Processing CSV file: Doc_file_Jsonl\\File_loc\\File_loc_batch_10.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling Doc_file_Jsonl\\File_loc\\File_loc_batch_10.csv: 100%|██████████| 15000/15000 [24:58<00:00, 10.01it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Crawl xong Doc_file_Jsonl\\File_loc\\File_loc_batch_10.csv!\n",
      "\n",
      "📄 Processing CSV file: Doc_file_Jsonl\\File_loc\\File_loc_batch_11.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling Doc_file_Jsonl\\File_loc\\File_loc_batch_11.csv: 100%|██████████| 15000/15000 [23:19<00:00, 10.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Crawl xong Doc_file_Jsonl\\File_loc\\File_loc_batch_11.csv!\n",
      "\n",
      "📄 Processing CSV file: Doc_file_Jsonl\\File_loc\\File_loc_batch_12.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling Doc_file_Jsonl\\File_loc\\File_loc_batch_12.csv: 100%|██████████| 15000/15000 [21:04<00:00, 11.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Crawl xong Doc_file_Jsonl\\File_loc\\File_loc_batch_12.csv!\n",
      "\n",
      "📄 Processing CSV file: Doc_file_Jsonl\\File_loc\\File_loc_batch_13.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling Doc_file_Jsonl\\File_loc\\File_loc_batch_13.csv: 100%|██████████| 15000/15000 [19:57<00:00, 12.53it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Crawl xong Doc_file_Jsonl\\File_loc\\File_loc_batch_13.csv!\n",
      "\n",
      "📄 Processing CSV file: Doc_file_Jsonl\\File_loc\\File_loc_batch_14.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling Doc_file_Jsonl\\File_loc\\File_loc_batch_14.csv: 100%|██████████| 15000/15000 [18:57<00:00, 13.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Crawl xong Doc_file_Jsonl\\File_loc\\File_loc_batch_14.csv!\n",
      "\n",
      "📄 Processing CSV file: Doc_file_Jsonl\\File_loc\\File_loc_batch_15.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling Doc_file_Jsonl\\File_loc\\File_loc_batch_15.csv: 100%|██████████| 15000/15000 [19:52<00:00, 12.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Crawl xong Doc_file_Jsonl\\File_loc\\File_loc_batch_15.csv!\n",
      "\n",
      "📄 Processing CSV file: Doc_file_Jsonl\\File_loc\\File_loc_batch_16.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling Doc_file_Jsonl\\File_loc\\File_loc_batch_16.csv: 100%|██████████| 15000/15000 [24:41<00:00, 10.12it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Crawl xong Doc_file_Jsonl\\File_loc\\File_loc_batch_16.csv!\n",
      "\n",
      "📄 Processing CSV file: Doc_file_Jsonl\\File_loc\\File_loc_batch_17.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling Doc_file_Jsonl\\File_loc\\File_loc_batch_17.csv: 100%|██████████| 15000/15000 [23:29<00:00, 10.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Crawl xong Doc_file_Jsonl\\File_loc\\File_loc_batch_17.csv!\n",
      "\n",
      "📄 Processing CSV file: Doc_file_Jsonl\\File_loc\\File_loc_batch_18.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling Doc_file_Jsonl\\File_loc\\File_loc_batch_18.csv: 100%|██████████| 15000/15000 [23:29<00:00, 10.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Crawl xong Doc_file_Jsonl\\File_loc\\File_loc_batch_18.csv!\n",
      "\n",
      "📄 Processing CSV file: Doc_file_Jsonl\\File_loc\\File_loc_batch_19.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling Doc_file_Jsonl\\File_loc\\File_loc_batch_19.csv: 100%|██████████| 15000/15000 [23:30<00:00, 10.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Crawl xong Doc_file_Jsonl\\File_loc\\File_loc_batch_19.csv!\n",
      "\n",
      "📄 Processing CSV file: Doc_file_Jsonl\\File_loc\\File_loc_batch_2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling Doc_file_Jsonl\\File_loc\\File_loc_batch_2.csv: 100%|██████████| 15000/15000 [23:47<00:00, 10.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Crawl xong Doc_file_Jsonl\\File_loc\\File_loc_batch_2.csv!\n",
      "\n",
      "📄 Processing CSV file: Doc_file_Jsonl\\File_loc\\File_loc_batch_20.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling Doc_file_Jsonl\\File_loc\\File_loc_batch_20.csv: 100%|██████████| 15000/15000 [23:30<00:00, 10.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Crawl xong Doc_file_Jsonl\\File_loc\\File_loc_batch_20.csv!\n",
      "\n",
      "📄 Processing CSV file: Doc_file_Jsonl\\File_loc\\File_loc_batch_21.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling Doc_file_Jsonl\\File_loc\\File_loc_batch_21.csv: 100%|██████████| 9509/9509 [14:52<00:00, 10.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Crawl xong Doc_file_Jsonl\\File_loc\\File_loc_batch_21.csv!\n",
      "\n",
      "📄 Processing CSV file: Doc_file_Jsonl\\File_loc\\File_loc_batch_3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling Doc_file_Jsonl\\File_loc\\File_loc_batch_3.csv: 100%|██████████| 15000/15000 [23:19<00:00, 10.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Crawl xong Doc_file_Jsonl\\File_loc\\File_loc_batch_3.csv!\n",
      "\n",
      "📄 Processing CSV file: Doc_file_Jsonl\\File_loc\\File_loc_batch_4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling Doc_file_Jsonl\\File_loc\\File_loc_batch_4.csv: 100%|██████████| 15000/15000 [23:23<00:00, 10.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Crawl xong Doc_file_Jsonl\\File_loc\\File_loc_batch_4.csv!\n",
      "\n",
      "📄 Processing CSV file: Doc_file_Jsonl\\File_loc\\File_loc_batch_5.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling Doc_file_Jsonl\\File_loc\\File_loc_batch_5.csv: 100%|██████████| 15000/15000 [23:31<00:00, 10.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Crawl xong Doc_file_Jsonl\\File_loc\\File_loc_batch_5.csv!\n",
      "\n",
      "📄 Processing CSV file: Doc_file_Jsonl\\File_loc\\File_loc_batch_6.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling Doc_file_Jsonl\\File_loc\\File_loc_batch_6.csv: 100%|██████████| 15000/15000 [23:19<00:00, 10.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Crawl xong Doc_file_Jsonl\\File_loc\\File_loc_batch_6.csv!\n",
      "\n",
      "📄 Processing CSV file: Doc_file_Jsonl\\File_loc\\File_loc_batch_7.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling Doc_file_Jsonl\\File_loc\\File_loc_batch_7.csv: 100%|██████████| 15000/15000 [23:14<00:00, 10.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Crawl xong Doc_file_Jsonl\\File_loc\\File_loc_batch_7.csv!\n",
      "\n",
      "📄 Processing CSV file: Doc_file_Jsonl\\File_loc\\File_loc_batch_8.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling Doc_file_Jsonl\\File_loc\\File_loc_batch_8.csv: 100%|██████████| 15000/15000 [23:02<00:00, 10.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Crawl xong Doc_file_Jsonl\\File_loc\\File_loc_batch_8.csv!\n",
      "\n",
      "📄 Processing CSV file: Doc_file_Jsonl\\File_loc\\File_loc_batch_9.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling Doc_file_Jsonl\\File_loc\\File_loc_batch_9.csv: 100%|██████████| 15000/15000 [23:08<00:00, 10.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Crawl xong Doc_file_Jsonl\\File_loc\\File_loc_batch_9.csv!\n",
      "✅ Crawl xong toàn bộ các file CSV!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ==== Cấu hình ====\n",
    "START_DATE = '20200101'\n",
    "END_DATE = '20241231'\n",
    "LANG = 'en'\n",
    "MAX_CONCURRENT = 12\n",
    "REQUESTS_PER_MINUTE = 100\n",
    "BATCH_SIZE = 100\n",
    "# ===================\n",
    "\n",
    "import os\n",
    "import time\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "import gc\n",
    "from aiohttp import TCPConnector\n",
    "from tqdm.asyncio import tqdm\n",
    "\n",
    "# Bộ giới hạn tốc độ\n",
    "class RateLimiter:\n",
    "    def __init__(self, calls: int, period: float):\n",
    "        self.calls = calls\n",
    "        self.period = period\n",
    "        self.requests = []\n",
    "    \n",
    "    async def __aenter__(self):\n",
    "        now = time.time()\n",
    "        self.requests = [t for t in self.requests if now - t < self.period]\n",
    "        if len(self.requests) >= self.calls:\n",
    "            await asyncio.sleep(self.period - (now - self.requests[0]))\n",
    "        self.requests.append(time.time())\n",
    "        return self\n",
    "    \n",
    "    async def __aexit__(self, exc_type, exc, tb):\n",
    "        pass\n",
    "\n",
    "async def get_pageviews(session: aiohttp.ClientSession, article_title: str, start_date: str, end_date: str, lang: str, max_retry: int = 3) -> list:\n",
    "    article_slug = article_title.replace(\" \", \"_\")\n",
    "    url = f\"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/{lang}.wikipedia.org/all-access/user/{article_slug}/daily/{start_date}/{end_date}\"\n",
    "    headers = {\"User-Agent\": \"MyWikiCrawler/1.0 (+mailto:cgdlddhm@gmail.com)\"}\n",
    "    retries = 0\n",
    "    while retries < max_retry:\n",
    "        async with RateLimiter(calls=REQUESTS_PER_MINUTE, period=60):\n",
    "            try:\n",
    "                async with session.get(url, headers=headers, timeout=10) as response:\n",
    "                    if response.status == 200:\n",
    "                        data = (await response.json()).get('items', [])\n",
    "                        return [{\n",
    "                            'title': article_title,\n",
    "                            'date': item['timestamp'][:8],\n",
    "                            'views': item['views']\n",
    "                        } for item in data]\n",
    "                    elif response.status == 429:\n",
    "                        wait_time = int(response.headers.get(\"Retry-After\", 10))\n",
    "                        await asyncio.sleep(wait_time)\n",
    "                        retries += 1\n",
    "                    else:\n",
    "                        return None\n",
    "            except Exception as e:\n",
    "                await asyncio.sleep(5)\n",
    "                retries += 1\n",
    "    return None\n",
    "\n",
    "async def crawl_one(session: aiohttp.ClientSession, title: str) -> tuple:\n",
    "    result = await get_pageviews(session, title, start_date=START_DATE, end_date=END_DATE, lang=LANG)\n",
    "    return title, result\n",
    "\n",
    "async def crawl_for_csv(csv_file: str, batch_number: int):\n",
    "    crawl_output_dir = os.path.join(\"Crawl(2)\", \"Crawl_raw\")\n",
    "    os.makedirs(crawl_output_dir, exist_ok=True)\n",
    "\n",
    "    output_csv = os.path.join(crawl_output_dir, f\"Crawl_raw_batch_{batch_number}.csv\")\n",
    "\n",
    "    try:\n",
    "        df_filtered = pd.read_csv(csv_file)\n",
    "    except Exception as e:\n",
    "        return\n",
    "    \n",
    "    titles = df_filtered['name'].dropna().unique().tolist()\n",
    "    if not titles:\n",
    "        return\n",
    "\n",
    "    connector = TCPConnector(limit=MAX_CONCURRENT, force_close=True)\n",
    "    semaphore = asyncio.Semaphore(MAX_CONCURRENT)\n",
    "\n",
    "    async with aiohttp.ClientSession(connector=connector) as session:\n",
    "        async def limited_crawl(title):\n",
    "            async with semaphore:\n",
    "                return await crawl_one(session, title)\n",
    "\n",
    "        tasks = [limited_crawl(title) for title in titles]\n",
    "        results = []\n",
    "\n",
    "        for i, future in enumerate(tqdm(asyncio.as_completed(tasks), total=len(tasks), desc=f\"Crawling {csv_file}\")):\n",
    "            title, result = await future\n",
    "            if result:\n",
    "                results.append(pd.DataFrame(result))\n",
    "\n",
    "            if (i + 1) % BATCH_SIZE == 0 or i == len(tasks) - 1:\n",
    "                if results:\n",
    "                    batch_df = pd.concat(results, ignore_index=True)\n",
    "                    if os.path.exists(output_csv):\n",
    "                        batch_df.to_csv(output_csv, mode='a', index=False, header=False)\n",
    "                    else:\n",
    "                        batch_df.to_csv(output_csv, index=False)\n",
    "\n",
    "                    del results\n",
    "                    del batch_df\n",
    "                    results = []\n",
    "                    gc.collect()\n",
    "\n",
    "    print(f\"✅ Crawl xong {csv_file}!\")\n",
    "\n",
    "async def main():\n",
    "    filtered_dir = os.path.join(\"Doc_file_Jsonl\", \"File_loc\")\n",
    "    if not os.path.exists(filtered_dir):\n",
    "        return\n",
    "\n",
    "    csv_files = [os.path.join(filtered_dir, f) for f in os.listdir(filtered_dir) if f.endswith(\".csv\")]\n",
    "    if not csv_files:\n",
    "        return\n",
    "    \n",
    "    for batch_number, csv_file in enumerate(csv_files, 1):\n",
    "        print(f\"\\n📄 Processing CSV file: {csv_file}\")\n",
    "        await crawl_for_csv(csv_file, batch_number)\n",
    "    \n",
    "    print(\"✅ Crawl xong toàn bộ các file CSV!\")\n",
    "\n",
    "# === Chạy trong Jupyter Notebook hoặc Script ===\n",
    "await main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
